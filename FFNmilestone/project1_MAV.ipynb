{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculadora -10 a 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n"
     ]
    }
   ],
   "source": [
    "# Crear un dataset de 1000 filas por 2 columnas. Cada columna contiene enteros aleatorios entre -10 y 10\n",
    "data = np.random.randint(-10, 10, (100, 2))\n",
    "\n",
    "# Crear un dataset de 1000 filas por 1 columna. Cada columna contiene la suma de los valores de las columnas de X\n",
    "labels = np.sum(data, axis=1)\n",
    "\n",
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: torch.Size([90, 2]), train_labels: torch.Size([90])\n",
      "test_data: torch.Size([10, 2]), test_labels: torch.Size([10])\n",
      "tensor([[ 6.,  4.],\n",
      "        [ 1.,  6.],\n",
      "        [ 8.,  3.],\n",
      "        [-5., -4.],\n",
      "        [ 3.,  7.]])\n",
      "tensor([10,  7, 11, -9, 10])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: convert to tensor\n",
    "dataT   = torch.tensor( data ).float()\n",
    "labelsT = torch.tensor( labels ).long()\n",
    "\n",
    "# Step 2: use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
    "\n",
    "print(f\"train_data: {train_data.shape}, train_labels: {train_labels.shape}\")\n",
    "print(f\"test_data: {test_data.shape}, test_labels: {test_labels.shape}\")\n",
    "\n",
    "# show the first rows of the data and the labels\n",
    "print(train_data[:5])\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 2]) torch.Size([90])\n",
      "[[ 6.  4. 10.]\n",
      " [ 1.  6.  7.]\n",
      " [ 8.  3. 11.]\n",
      " [-5. -4. -9.]\n",
      " [ 3.  7. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# REMINDER:\n",
    "# 0) normalize the data\n",
    "# train_dataN = (train_data - train_data.mean()) / train_data.std()\n",
    "# test_dataN  = (test_data - test_data.mean()) / test_data.std()\n",
    "\n",
    "train_dataN = train_data\n",
    "test_dataN  = test_data\n",
    "\n",
    "print(train_dataN.shape, train_labels.shape)\n",
    "\n",
    "# show the first rows of train_dataN and train_labels\n",
    "print(np.hstack((train_dataN[:5], train_labels[:5].reshape(-1,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 2])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: convert into PyTorch Datasets\n",
    "train_dataTds = TensorDataset(train_dataN,train_labels)\n",
    "test_dataTds  = TensorDataset(test_dataN,test_labels)\n",
    "\n",
    "# Step 4: translate into dataloader objects\n",
    "batchsize    = 32\n",
    "train_loader = DataLoader(train_dataTds,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_dataTds,batch_size=test_dataTds.tensors[0].shape[0])\n",
    "\n",
    "# show the dataloader object and the shape of the data\n",
    "print(train_loader.dataset.tensors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de pérdida en PyTorch\n",
    "![Funciones de pérdida en PyTorch](../_USEFUL_TABLES/loss_functions_pytorch_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizadores en PyTorch\n",
    "![optimizadores en PyTorch](../_USEFUL_TABLES/optimizers_pytorch_optim.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheModel():\n",
    "\n",
    "  class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(2,4)\n",
    "      \n",
    "      ### hidden layer\n",
    "      self.fc1 = nn.Linear(4,4)\n",
    "      self.fc2 = nn.Linear(4,4)\n",
    "\n",
    "      ### output layer\n",
    "      self.output = nn.Linear(4,1)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      x = F.relu( self.fc1(x) )\n",
    "      x = F.relu( self.fc2(x) )\n",
    "      return self.output(x)\n",
    "  \n",
    "  # create the model instance\n",
    "  net = Net()\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(net.parameters(), lr=.01)  # Simple and efficient, especially for large-scale learning and convex problems. General problems (typically for large datasets)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "def trainTheModel():\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 10\n",
    "  \n",
    "  # create a new model\n",
    "  net, lossfun, optimizer = createTheModel()\n",
    "\n",
    "  # initialize losses\n",
    "  losses    = torch.zeros((numepochs, 2))\n",
    "  trainAcc  = []\n",
    "  testAcc   = []\n",
    "  \n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    \n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      print(f'X: {X}')\n",
    "      print(f'yHat: {yHat}')\n",
    "      print(f'y: {y}')\n",
    "      loss = lossfun(yHat, y)  # lossfun returns a Tensor with the loss\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())  # .item() returns the scalar value held in the loss tensor\n",
    "\n",
    "      # compute accuracy\n",
    "      print(f'y: {y}')\n",
    "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
    "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    losses[epochi, 0] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(X)\n",
    "      \n",
    "    # compare the following really long line of code to the training accuracy lines\n",
    "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "    loss = lossfun(yHat,y)\n",
    "    losses[epochi, 1] = loss.item()\n",
    "\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses,net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model and show the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[  8.,  -6.],\n",
      "        [ -6., -10.],\n",
      "        [  1.,   6.],\n",
      "        [  9.,  -4.],\n",
      "        [  1.,  -7.],\n",
      "        [ -3.,   0.],\n",
      "        [  0.,   0.],\n",
      "        [  6.,   0.],\n",
      "        [  1.,   7.],\n",
      "        [ -9.,  -3.],\n",
      "        [  0.,  -9.],\n",
      "        [-10.,  -4.],\n",
      "        [  8.,  -9.],\n",
      "        [  9.,  -2.],\n",
      "        [  5.,   3.],\n",
      "        [  4.,   9.],\n",
      "        [ -8.,  -9.],\n",
      "        [  7.,  -6.],\n",
      "        [  3.,  -4.],\n",
      "        [ -6.,  -5.],\n",
      "        [ -3.,   3.],\n",
      "        [ -7.,   1.],\n",
      "        [ -1.,   9.],\n",
      "        [  4.,   1.],\n",
      "        [ -2.,   5.],\n",
      "        [ -7.,   7.],\n",
      "        [ -3.,  -6.],\n",
      "        [ -6.,  -8.],\n",
      "        [ -7.,  -1.],\n",
      "        [  7.,   3.],\n",
      "        [  6.,  -5.],\n",
      "        [  1.,  -8.]])\n",
      "yHat: tensor([[0.6485],\n",
      "        [0.4825],\n",
      "        [0.5095],\n",
      "        [0.6433],\n",
      "        [0.4584],\n",
      "        [0.4706],\n",
      "        [0.4689],\n",
      "        [0.5047],\n",
      "        [0.5066],\n",
      "        [0.5079],\n",
      "        [0.4109],\n",
      "        [0.5161],\n",
      "        [0.6260],\n",
      "        [0.5860],\n",
      "        [0.5378],\n",
      "        [0.5846],\n",
      "        [0.5448],\n",
      "        [0.6239],\n",
      "        [0.5367],\n",
      "        [0.5256],\n",
      "        [0.4706],\n",
      "        [0.4706],\n",
      "        [0.4706],\n",
      "        [0.5058],\n",
      "        [0.4706],\n",
      "        [0.4706],\n",
      "        [0.4609],\n",
      "        [0.5086],\n",
      "        [0.4706],\n",
      "        [0.5498],\n",
      "        [0.6049],\n",
      "        [0.4491]], grad_fn=<AddmmBackward0>)\n",
      "y: tensor([  2, -16,   7,   5,  -6,  -3,   0,   6,   8, -12,  -9, -14,  -1,   7,\n",
      "          8,  13, -17,   1,  -1, -11,   0,  -6,   8,   5,   3,   0,  -9, -14,\n",
      "         -8,  10,   1,  -7])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[259], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainAcc,testAcc,losses,net \u001b[38;5;241m=\u001b[39m \u001b[43mtrainTheModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[258], line 31\u001b[0m, in \u001b[0;36mtrainTheModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myHat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myHat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mlossfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43myHat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lossfun returns a Tensor with the loss\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# backprop\u001b[39;00m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Python\\UDEMY - DeepUnderstandingOfDeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\UDEMY - DeepUnderstandingOfDeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\UDEMY - DeepUnderstandingOfDeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\UDEMY - DeepUnderstandingOfDeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "trainAcc,testAcc,losses,net = trainTheModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data range -1.67033 to 1.64879\n",
      "Test data range -1.51955 to 1.74645\n"
     ]
    }
   ],
   "source": [
    "# Confirm ranges of train and test data\n",
    "\n",
    "print('Training data range %g to %g' \n",
    "      %(torch.min(train_loader.dataset.tensors[0]),torch.max(train_loader.dataset.tensors[0])) )\n",
    "\n",
    "print('Test data range %g to %g' \n",
    "      %(torch.min(test_loader.dataset.tensors[0]),torch.max(test_loader.dataset.tensors[0])) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
